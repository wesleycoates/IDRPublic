{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Model Building\n",
    "\n",
    "This notebook guides you through the process of exploring your data and building a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from typing import Optional, List, Dict, Any, Tuple\n",
      "import pickle\n",
      "from pathlib import Path\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
      "from sklearn.metrics import (\n",
      "    mean_squared_error, mean_absolute_error, r2_score,  # Regression metrics\n",
      "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,  # Classification metrics\n",
      "    roc_auc_score, roc_curve, precision_recall_curve\n",
      ")\n",
      "\n",
      "# Import various models\n",
      "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
      "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
      "from sklearn.svm import SVR, SVC\n",
      "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
      "\n",
      "\n",
      "def get_numeric_and_categorical_columns(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
      "    \"\"\"\n",
      "    Identify numeric and categorical columns in a DataFrame.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    df : pd.DataFrame\n",
      "        The data to analyze\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    tuple\n",
      "        Lists of numeric and categorical column names\n",
      "    \"\"\"\n",
      "    # Identify numeric columns (excluding any obvious target/ID columns)\n",
      "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
      "    \n",
      "    # Identify categorical columns - including object, category and boolean types\n",
      "    categorical_columns = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
      "    \n",
      "    return numeric_columns, categorical_columns\n",
      "\n",
      "\n",
      "def create_preprocessing_pipeline(numeric_columns: List[str], categorical_columns: List[str]) -> ColumnTransformer:\n",
      "    \"\"\"\n",
      "    Create a preprocessing pipeline for numeric and categorical features.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    numeric_columns : list of str\n",
      "        Names of numeric columns\n",
      "    categorical_columns : list of str\n",
      "        Names of categorical columns\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    ColumnTransformer\n",
      "        Preprocessing pipeline\n",
      "    \"\"\"\n",
      "    # Define numeric preprocessing pipeline\n",
      "    numeric_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='median')),\n",
      "        ('scaler', StandardScaler())\n",
      "    ])\n",
      "    \n",
      "    # Define categorical preprocessing pipeline\n",
      "    categorical_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
      "    ])\n",
      "    \n",
      "    # Combine preprocessing steps\n",
      "    preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            ('num', numeric_transformer, numeric_columns),\n",
      "            ('cat', categorical_transformer, categorical_columns)\n",
      "        ]\n",
      "    )\n",
      "    \n",
      "    return preprocessor\n",
      "\n",
      "\n",
      "def get_regression_models() -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Get a dictionary of regression models.\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    dict\n",
      "        Dictionary with model names as keys and model instances as values\n",
      "    \"\"\"\n",
      "    return {\n",
      "        'Linear Regression': LinearRegression(),\n",
      "        'Ridge Regression': Ridge(),\n",
      "        'Lasso Regression': Lasso(),\n",
      "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
      "        'Random Forest': RandomForestRegressor(random_state=42),\n",
      "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
      "        'SVR': SVR(),\n",
      "        'KNN': KNeighborsRegressor()\n",
      "    }\n",
      "\n",
      "\n",
      "def get_classification_models() -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Get a dictionary of classification models.\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    dict\n",
      "        Dictionary with model names as keys and model instances as values\n",
      "    \"\"\"\n",
      "    return {\n",
      "        'Logistic Regression': LogisticRegression(random_state=42),\n",
      "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
      "        'Random Forest': RandomForestClassifier(random_state=42),\n",
      "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
      "        'SVC': SVC(probability=True, random_state=42),\n",
      "        'KNN': KNeighborsClassifier()\n",
      "    }\n",
      "\n",
      "\n",
      "def evaluate_regression_models(X_train: pd.DataFrame, y_train: pd.Series, \n",
      "                              X_test: pd.DataFrame, y_test: pd.Series,\n",
      "                              preprocessor: ColumnTransformer) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Evaluate multiple regression models and return their performance metrics.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    X_train : pd.DataFrame\n",
      "        Training features\n",
      "    y_train : pd.Series\n",
      "        Training target\n",
      "    X_test : pd.DataFrame\n",
      "        Testing features\n",
      "    y_test : pd.Series\n",
      "        Testing target\n",
      "    preprocessor : ColumnTransformer\n",
      "        Preprocessing pipeline\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    pd.DataFrame\n",
      "        Performance metrics for each model\n",
      "    \"\"\"\n",
      "    # Get regression models\n",
      "    models = get_regression_models()\n",
      "    \n",
      "    # Results dictionary\n",
      "    results = []\n",
      "    \n",
      "    for name, model in models.items():\n",
      "        # Create pipeline with preprocessor and model\n",
      "        pipeline = Pipeline(steps=[\n",
      "            ('preprocessor', preprocessor),\n",
      "            ('model', model)\n",
      "        ])\n",
      "        \n",
      "        # Fit model\n",
      "        pipeline.fit(X_train, y_train)\n",
      "        \n",
      "        # Make predictions\n",
      "        y_pred = pipeline.predict(X_test)\n",
      "        \n",
      "        # Calculate metrics\n",
      "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
      "        mae = mean_absolute_error(y_test, y_pred)\n",
      "        r2 = r2_score(y_test, y_pred)\n",
      "        \n",
      "        # Calculate cross-validation score\n",
      "        cv_score = np.mean(cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2'))\n",
      "        \n",
      "        # Store results\n",
      "        results.append({\n",
      "            'Model': name,\n",
      "            'RMSE': rmse,\n",
      "            'MAE': mae,\n",
      "            'R²': r2,\n",
      "            'CV R²': cv_score\n",
      "        })\n",
      "    \n",
      "    # Convert to DataFrame and sort by R²\n",
      "    results_df = pd.DataFrame(results)\n",
      "    results_df = results_df.sort_values('R²', ascending=False).reset_index(drop=True)\n",
      "    \n",
      "    return results_df\n",
      "\n",
      "\n",
      "def evaluate_classification_models(X_train: pd.DataFrame, y_train: pd.Series, \n",
      "                                  X_test: pd.DataFrame, y_test: pd.Series,\n",
      "                                  preprocessor: ColumnTransformer) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Evaluate multiple classification models and return their performance metrics.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    X_train : pd.DataFrame\n",
      "        Training features\n",
      "    y_train : pd.Series\n",
      "        Training target\n",
      "    X_test : pd.DataFrame\n",
      "        Testing features\n",
      "    y_test : pd.Series\n",
      "        Testing target\n",
      "    preprocessor : ColumnTransformer\n",
      "        Preprocessing pipeline\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    pd.DataFrame\n",
      "        Performance metrics for each model\n",
      "    \"\"\"\n",
      "    # Get classification models\n",
      "    models = get_classification_models()\n",
      "    \n",
      "    # Results dictionary\n",
      "    results = []\n",
      "    \n",
      "    for name, model in models.items():\n",
      "        # Create pipeline with preprocessor and model\n",
      "        pipeline = Pipeline(steps=[\n",
      "            ('preprocessor', preprocessor),\n",
      "            ('model', model)\n",
      "        ])\n",
      "        \n",
      "        # Fit model\n",
      "        pipeline.fit(X_train, y_train)\n",
      "        \n",
      "        # Make predictions\n",
      "        y_pred = pipeline.predict(X_test)\n",
      "        \n",
      "        # For ROC-AUC, we need probability predictions\n",
      "        try:\n",
      "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
      "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
      "        except:\n",
      "            roc_auc = np.nan\n",
      "        \n",
      "        # Calculate metrics\n",
      "        accuracy = accuracy_score(y_test, y_pred)\n",
      "        \n",
      "        # For multi-class problems, we need to specify an average method\n",
      "        if len(np.unique(y_train)) > 2:\n",
      "            precision = precision_score(y_test, y_pred, average='weighted')\n",
      "            recall = recall_score(y_test, y_pred, average='weighted')\n",
      "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
      "        else:\n",
      "            precision = precision_score(y_test, y_pred)\n",
      "            recall = recall_score(y_test, y_pred)\n",
      "            f1 = f1_score(y_test, y_pred)\n",
      "        \n",
      "        # Calculate cross-validation score\n",
      "        cv_score = np.mean(cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy'))\n",
      "        \n",
      "        # Store results\n",
      "        results.append({\n",
      "            'Model': name,\n",
      "            'Accuracy': accuracy,\n",
      "            'Precision': precision,\n",
      "            'Recall': recall,\n",
      "            'F1 Score': f1,\n",
      "            'ROC-AUC': roc_auc,\n",
      "            'CV Accuracy': cv_score\n",
      "        })\n",
      "    \n",
      "    # Convert to DataFrame and sort by F1 Score\n",
      "    results_df = pd.DataFrame(results)\n",
      "    results_df = results_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
      "    \n",
      "    return results_df\n",
      "\n",
      "\n",
      "def tune_hyperparameters(X_train: pd.DataFrame, y_train: pd.Series,\n",
      "                        model, param_grid: Dict[str, Any],\n",
      "                        preprocessor: ColumnTransformer,\n",
      "                        cv: int = 5, scoring: str = None):\n",
      "    \"\"\"\n",
      "    Tune hyperparameters for a given model using GridSearchCV.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    X_train : pd.DataFrame\n",
      "        Training features\n",
      "    y_train : pd.Series\n",
      "        Training target\n",
      "    model : estimator\n",
      "        The model to tune\n",
      "    param_grid : dict\n",
      "        Parameter grid for hyperparameter tuning\n",
      "    preprocessor : ColumnTransformer\n",
      "        Preprocessing pipeline\n",
      "    cv : int, default=5\n",
      "        Number of cross-validation folds\n",
      "    scoring : str, optional\n",
      "        Scoring metric to use\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    tuple\n",
      "        Best estimator and a DataFrame with detailed CV results\n",
      "    \"\"\"\n",
      "    # Create pipeline with preprocessor and model\n",
      "    pipeline = Pipeline(steps=[\n",
      "        ('preprocessor', preprocessor),\n",
      "        ('model', model)\n",
      "    ])\n",
      "    \n",
      "    # Create parameter grid for the pipeline\n",
      "    pipeline_param_grid = {f'model__{param}': values for param, values in param_grid.items()}\n",
      "    \n",
      "    # Create grid search\n",
      "    grid_search = GridSearchCV(\n",
      "        pipeline,\n",
      "        param_grid=pipeline_param_grid,\n",
      "        cv=cv,\n",
      "        scoring=scoring,\n",
      "        n_jobs=-1,\n",
      "        verbose=1\n",
      "    )\n",
      "    \n",
      "    # Fit grid search\n",
      "    grid_search.fit(X_train, y_train)\n",
      "    \n",
      "    # Convert CV results to DataFrame\n",
      "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
      "    \n",
      "    # Get only relevant columns\n",
      "    cv_results = cv_results[[col for col in cv_results.columns if 'param_' in col or 'mean_test_score' in col or 'std_test_score' in col]]\n",
      "    \n",
      "    # Rename parameter columns\n",
      "    cv_results.columns = [col.replace('param_model__', '') if 'param_model__' in col else col for col in cv_results.columns]\n",
      "    \n",
      "    # Sort by mean test score\n",
      "    cv_results = cv_results.sort_values('mean_test_score', ascending=False).reset_index(drop=True)\n",
      "    \n",
      "    return grid_search.best_estimator_, cv_results\n",
      "\n",
      "\n",
      "def save_model(model, filepath: str):\n",
      "    \"\"\"\n",
      "    Save a trained model to disk.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    model : estimator\n",
      "        Trained model to save\n",
      "    filepath : str\n",
      "        Path where the model will be saved\n",
      "    \"\"\"\n",
      "    # Create directory if it doesn't exist\n",
      "    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
      "    \n",
      "    # Save model\n",
      "    with open(filepath, 'wb') as f:\n",
      "        pickle.dump(model, f)\n",
      "    \n",
      "    print(f\"Model saved to {filepath}\")\n",
      "\n",
      "\n",
      "def load_model(filepath: str):\n",
      "    \"\"\"\n",
      "    Load a trained model from disk.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    filepath : str\n",
      "        Path where the model is saved\n",
      "        \n",
      "    Returns:\n",
      "    --------\n",
      "    The loaded model\n",
      "    \"\"\"\n",
      "    with open(filepath, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "    \n",
      "    print(f\"Model loaded from {filepath}\")\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # This will execute when you run this script directly\n",
      "    print(\"This is a module for building and evaluating predictive models.\")\n",
      "    print(\"Example usage:\")\n",
      "    print(\"from modeling import evaluate_regression_models, evaluate_classification_models\")\n",
      "    print(\"results = evaluate_regression_models(X_train, y_train, X_test, y_test, preprocessor)\")\n"
     ]
    }
   ],
   "source": [
    "# To inspect the file\n",
    "with open('../src/modeling.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for modeling.py at: /workspaces/IDRPublic/src/modeling.py\n",
      "File exists: True\n",
      "First few lines of the file:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from typing import Optional, List, Dict, Any, Tuple\n",
      "import pickle\n",
      "from pathlib import Path\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
      "from sklearn.metrics import (\n",
      "    mean_squared_error, mean_absolute_error, r2_score,  # Regression metrics\n",
      "    acc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the exact path being used\n",
    "expected_path = os.path.abspath(os.path.join('..', 'src', 'modeling.py'))\n",
    "print(f\"Looking for modeling.py at: {expected_path}\")\n",
    "print(f\"File exists: {os.path.exists(expected_path)}\")\n",
    "\n",
    "# Read the first few lines to confirm it's the correct file\n",
    "if os.path.exists(expected_path):\n",
    "    with open(expected_path, 'r') as f:\n",
    "        content = f.read(500)  # Read first 500 characters\n",
    "        print(\"First few lines of the file:\")\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function exists in direct import: True\n",
      "Function signature: {'df': <class 'pandas.core.frame.DataFrame'>, 'return': typing.Tuple[typing.List[str], typing.List[str]]}\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load the module directly from the file path\n",
    "expected_path = '/workspaces/IDRPublic/src/modeling.py'\n",
    "spec = importlib.util.spec_from_file_location(\"modeling_direct\", expected_path)\n",
    "modeling_direct = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"modeling_direct\"] = modeling_direct\n",
    "spec.loader.exec_module(modeling_direct)\n",
    "\n",
    "# Check if the function exists in the loaded module\n",
    "print(\"Function exists in direct import:\", hasattr(modeling_direct, \"get_numeric_and_categorical_columns\"))\n",
    "\n",
    "# Try to access the function\n",
    "if hasattr(modeling_direct, \"get_numeric_and_categorical_columns\"):\n",
    "    print(\"Function signature:\", modeling_direct.get_numeric_and_categorical_columns.__annotations__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed functions directly from our manually loaded module\n",
    "get_numeric_and_categorical_columns = modeling_direct.get_numeric_and_categorical_columns\n",
    "create_preprocessing_pipeline = modeling_direct.create_preprocessing_pipeline\n",
    "evaluate_regression_models = modeling_direct.evaluate_regression_models\n",
    "evaluate_classification_models = modeling_direct.evaluate_classification_models\n",
    "\n",
    "# Test that the function works\n",
    "import pandas as pd\n",
    "test_df = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})\n",
    "num_cols, cat_cols = get_numeric_and_categorical_columns(test_df)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Import custom modules (adjust the path if needed)\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_loader import load_excel_data, clean_data, split_data\n",
    "from visualization import plot_numeric_distribution, plot_correlation_matrix, plot_feature_importance\n",
    "from modeling import (get_numeric_and_categorical_columns, create_preprocessing_pipeline,\n",
    "                      evaluate_regression_models, evaluate_classification_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data - replace with your actual file path\n",
    "file_path = \"../data/raw/2023-q1-federal-idr-puf.xlsx\"\n",
    "\n",
    "# Try to load the data\n",
    "try:\n",
    "    df = load_excel_data(file_path)\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please make sure the file exists and the path is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Let's clean the data and perform some basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_df = clean_data(df)\n",
    "\n",
    "# Display information about the cleaned data\n",
    "print(\"Data information:\")\n",
    "clean_df.info()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(clean_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "display(clean_df.isna().sum())\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols, categorical_cols = get_numeric_and_categorical_columns(clean_df)\n",
    "print(f\"\\nNumeric columns: {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Let's examine a sample of categorical columns if any exist\n",
    "if categorical_cols:\n",
    "    for col in categorical_cols[:3]:  # Show first 3 at most\n",
    "        print(f\"\\nUnique values in {col}:\")\n",
    "        display(clean_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Let's visualize the data to understand it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numeric features\n",
    "if numeric_cols:\n",
    "    print(\"Distribution of numeric features:\")\n",
    "    plot_numeric_distribution(clean_df, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"Correlation matrix:\")\n",
    "    plot_correlation_matrix(clean_df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting Target Variable\n",
    "\n",
    "Now, let's select the target variable for our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns for user to select as target\n",
    "print(\"Available columns for target variable:\")\n",
    "for i, col in enumerate(clean_df.columns):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# Select your target variable - change this to your actual target column name\n",
    "target_column = 'your_target_column'  # Replace with the actual column name\n",
    "\n",
    "# Check data type of target variable to determine if this is a regression or classification problem\n",
    "if target_column in clean_df.columns:\n",
    "    print(f\"\\nTarget variable: {target_column}\")\n",
    "    print(f\"Data type: {clean_df[target_column].dtype}\")\n",
    "    \n",
    "    # For numeric target, show distribution\n",
    "    if pd.api.types.is_numeric_dtype(clean_df[target_column]):\n",
    "        print(\"\\nTarget distribution:\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(clean_df[target_column], kde=True)\n",
    "        plt.title(f'Distribution of {target_column}')\n",
    "        plt.show()\n",
    "        \n",
    "        problem_type = 'regression'\n",
    "        print(\"\\nThis appears to be a regression problem.\")\n",
    "    else:\n",
    "        # For categorical target, show value counts\n",
    "        print(\"\\nTarget value counts:\")\n",
    "        display(clean_df[target_column].value_counts())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y=clean_df[target_column])\n",
    "        plt.title(f'Count of {target_column}')\n",
    "        plt.show()\n",
    "        \n",
    "        problem_type = 'classification'\n",
    "        print(\"\\nThis appears to be a classification problem.\")\n",
    "else:\n",
    "    print(f\"Error: '{target_column}' not found in the data. Please select a valid column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting\n",
    "\n",
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data - make sure to use your actual target column name\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = split_data(clean_df, target_column)\n",
    "    \n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Testing features shape: {X_test.shape}\")\n",
    "    print(f\"Training target shape: {y_train.shape}\")\n",
    "    print(f\"Testing target shape: {y_test.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please make sure you've selected a valid target column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Building and Evaluation\n",
    "\n",
    "Build and evaluate predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "numeric_cols, categorical_cols = get_numeric_and_categorical_columns(X_train)\n",
    "preprocessor = create_preprocessing_pipeline(numeric_cols, categorical_cols)\n",
    "\n",
    "# Evaluate models based on problem type\n",
    "if 'problem_type' in locals() and problem_type == 'regression':\n",
    "    # Evaluate regression models\n",
    "    print(\"Evaluating regression models...\")\n",
    "    regression_results = evaluate_regression_models(X_train, y_train, X_test, y_test, preprocessor)\n",
    "    display(regression_results)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='R²', y='Model', data=regression_results)\n",
    "    plt.title('Model Comparison - R² Score')\n",
    "    plt.xlim(0, 1)  # R² typically ranges from 0 to 1\n",
    "    plt.show()\n",
    "    \n",
    "elif 'problem_type' in locals() and problem_type == 'classification':\n",
    "    # Evaluate classification models\n",
    "    print(\"Evaluating classification models...\")\n",
    "    classification_results = evaluate_classification_models(X_train, y_train, X_test, y_test, preprocessor)\n",
    "    display(classification_results)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='F1 Score', y='Model', data=classification_results)\n",
    "    plt.title('Model Comparison - F1 Score')\n",
    "    plt.xlim(0, 1)  # F1 score ranges from 0 to 1\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Please run the previous cell to determine the problem type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance\n",
    "\n",
    "Let's examine which features are most important for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on problem type\n",
    "if 'problem_type' in locals() and problem_type == 'regression' and 'regression_results' in locals():\n",
    "    best_model_name = regression_results.iloc[0]['Model']\n",
    "    print(f\"Best regression model: {best_model_name}\")\n",
    "    \n",
    "    # For tree-based models, we can extract feature importance\n",
    "    if best_model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "        # Import and get models\n",
    "        from modeling import get_regression_models\n",
    "        models = get_regression_models()\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = models[best_model_name]\n",
    "        \n",
    "        # Create full pipeline\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', best_model)])\n",
    "        \n",
    "        # Fit model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = numeric_cols.copy()\n",
    "        # For categorical features, get one-hot encoded column names\n",
    "        for cat_col in categorical_cols:\n",
    "            unique_values = X_train[cat_col].unique()\n",
    "            for value in unique_values:\n",
    "                feature_names.append(f\"{cat_col}_{value}\")\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = pipeline.named_steps['model'].feature_importances_\n",
    "        \n",
    "        # Get the right number of feature names\n",
    "        if len(feature_names) > len(importances):\n",
    "            feature_names = feature_names[:len(importances)]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(feature_names, importances, title=f\"{best_model_name} - Feature Importance\")\n",
    "    \n",
    "elif 'problem_type' in locals() and problem_type == 'classification' and 'classification_results' in locals():\n",
    "    best_model_name = classification_results.iloc[0]['Model']\n",
    "    print(f\"Best classification model: {best_model_name}\")\n",
    "    \n",
    "    # For tree-based models, we can extract feature importance\n",
    "    if best_model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "        # Import and get models\n",
    "        from modeling import get_classification_models\n",
    "        models = get_classification_models()\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = models[best_model_name]\n",
    "        \n",
    "        # Create full pipeline\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', best_model)])\n",
    "        \n",
    "        # Fit model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = numeric_cols.copy()\n",
    "        # For categorical features, get one-hot encoded column names\n",
    "        for cat_col in categorical_cols:\n",
    "            unique_values = X_train[cat_col].unique()\n",
    "            for value in unique_values:\n",
    "                feature_names.append(f\"{cat_col}_{value}\")\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = pipeline.named_steps['model'].feature_importances_\n",
    "        \n",
    "        # Get the right number of feature names\n",
    "        if len(feature_names) > len(importances):\n",
    "            feature_names = feature_names[:len(importances)]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(feature_names, importances, title=f\"{best_model_name} - Feature Importance\")\n",
    "else:\n",
    "    print(\"Please run the model evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Best Model\n",
    "\n",
    "Save the best performing model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "if 'pipeline' in locals():\n",
    "    from modeling import save_model\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'../models/best_{problem_type}_model.pkl'\n",
    "    save_model(pipeline, model_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(\"Please run the model evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Here are some suggestions for next steps:\n",
    "\n",
    "1. **Feature Engineering**: Create new features or transform existing ones to improve model performance\n",
    "2. **Hyperparameter Tuning**: Fine-tune the best model to improve its performance\n",
    "3. **Model Interpretation**: Use tools like SHAP values to better understand model predictions\n",
    "4. **Cross-Validation**: Perform more robust model evaluation using cross-validation\n",
    "5. **Create a Prediction Pipeline**: Build a reusable pipeline for making predictions on new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
